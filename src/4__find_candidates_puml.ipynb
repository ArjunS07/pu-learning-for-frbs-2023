{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "import time\n",
    "from math import floor\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from utils import get_subburst_preserved_train_test, lee_liu_score\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "import ast\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from pulearn import (\n",
    "    ElkanotoPuClassifier,\n",
    "    WeightedElkanotoPuClassifier,\n",
    "    BaggingPuClassifier,\n",
    ")\n",
    "from pu_modified_lr.mlr import ModifiedLogisticRegression\n",
    "from PUExtraTrees.trees import PUExtraTrees\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import recall_score, fbeta_score\n",
    "import shap\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from umap import UMAP\n",
    "\n",
    "plotpar = {\"text.usetex\": True}\n",
    "plt.rcParams.update(plotpar)\n",
    "\n",
    "from utils import (\n",
    "    SUPERVISED_PAPER_REPEATERS,\n",
    "    UNSUPERVISED_PAPER_REPEATERS,\n",
    "    MORPHOLOGY_REPEATERS,\n",
    "    PCC_REPEATER_CANDIDATES,\n",
    "    REFERENCE_PAPER_REPEATERS,\n",
    ")\n",
    "from utils import custom_cmap\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/features_extracted/combined_2021_23_catalog.csv\")\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "FEATURES = [\n",
    "    \"ra\",\n",
    "    \"snr_fitb\",\n",
    "    \"log_dm_exc_ne2001\",\n",
    "    \"log_bc_width\",\n",
    "    \"log_flux\",\n",
    "    \"log_fluence\",\n",
    "    \"sp_idx\",\n",
    "    \"sp_run\",\n",
    "    \"log_in_duration\",\n",
    "    \"log_peak_freq\",\n",
    "    \"log_fre_width\",\n",
    "    \"log_T_B\",\n",
    "    \"log_energy\",\n",
    "]\n",
    "FEATURE_LABELS = [\n",
    "    \"Right Ascension\",\n",
    "    \"SNR (fitburst)\",\n",
    "    \"Extragalactic DM (NE2001)\",\n",
    "    \"Boxcar width\",\n",
    "    \"Flux\",\n",
    "    \"Fluence\",\n",
    "    \"Spectral index\",\n",
    "    \"Spectral running\",\n",
    "    \"Rest-frame width\",\n",
    "    \"Peak frequency\",\n",
    "    \"Frequency width\",\n",
    "    \"Brightness temperature\",\n",
    "    \"Burst energy\",\n",
    "]\n",
    "X = df[FEATURES]\n",
    "y = df[\"is_repeater\"]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read results from the previous notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervised_models_df = pd.read_csv(\"data/supervised_models_optimised.csv\")\n",
    "supervised_models_df.sort_values(\"recall_mean\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select models based on recall\n",
    "BASE_CLASSIFIERS = list(\n",
    "    supervised_models_df.sort_values(\"recall_mean\", ascending=False)[\"model\"][:3]\n",
    ")\n",
    "print(BASE_CLASSIFIERS)\n",
    "# 2 of our base classifiers are considered to be well-calibrated: their probability can be interpreted as a confidence level\n",
    "CALIBRATED_CLASSIFIERS = [\"LogisticRegression\", \"LinearDiscriminantAnalysis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a standard train-test split that will be used for optimisation\n",
    "np.random.seed(RANDOM_SEED)\n",
    "X_train, X_val, y_train, y_val = get_subburst_preserved_train_test(\n",
    "    original_df=df, X=X, y=y, test_size=0.2\n",
    ")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PU Classifier Optimisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRIALS = 150\n",
    "optimised_models = []  # store optimised model objects\n",
    "models_info = (\n",
    "    []\n",
    ")  # store information about each model, including optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_base_classifier(clf_name):\n",
    "    \"\"\"\n",
    "    Given a classifier name, find the optimal parameters saved in supervised_models_df.\n",
    "    Then, find the corresponding class for the model in the global scope, and instantiate that class using the parameters\n",
    "    \"\"\"\n",
    "    row = supervised_models_df[supervised_models_df[\"model\"] == clf_name].iloc[0]\n",
    "    params = ast.literal_eval(row[\"params\"])\n",
    "    base_clf = globals()[clf_name](**params)\n",
    "    return base_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elkanoto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "\n",
    "def elkanoto_objective(trial):\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    base_clf_option = trial.suggest_categorical(\"base_clf_option\", BASE_CLASSIFIERS)\n",
    "    best_classifier = best_base_classifier(base_clf_option)\n",
    "\n",
    "    # If necessary, calibrate the classifier using Platt scaling\n",
    "    if base_clf_option not in CALIBRATED_CLASSIFIERS:\n",
    "        best_classifier = CalibratedClassifierCV(\n",
    "            best_classifier, method=\"sigmoid\", cv=10\n",
    "        )\n",
    "    classifier_obj = ElkanotoPuClassifier(\n",
    "        estimator=best_classifier,\n",
    "        hold_out_ratio=trial.suggest_float(\"hold_out_ratio\", 0.1, 0.8),\n",
    "    )\n",
    "    # Fit and evaluate on the same sets of data each time\n",
    "    classifier_obj.fit(X_train_scaled, y_train)\n",
    "    y_pred = classifier_obj.predict(X_val_scaled)\n",
    "\n",
    "    # Maximise lee-liu score\n",
    "    return lee_liu_score(y_known=y_val, y_pred=y_pred)\n",
    "\n",
    "\n",
    "sampler = TPESampler(seed=RANDOM_SEED)\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "study.optimize(elkanoto_objective, n_trials=NUM_TRIALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best params and save. Re-instantiate the classifeir using those parameters and save it to a list\n",
    "best_params = study.best_params\n",
    "best_clf_option = best_params[\"base_clf_option\"]\n",
    "best_classifier = best_base_classifier(best_clf_option)\n",
    "if best_clf_option not in CALIBRATED_CLASSIFIERS:\n",
    "    print(\"Calibating...\")\n",
    "    best_classifier = CalibratedClassifierCV(best_classifier, method=\"sigmoid\", cv=10)\n",
    "optimised_elkanoto = ElkanotoPuClassifier(\n",
    "    estimator=best_classifier,\n",
    "    hold_out_ratio=best_params[\"hold_out_ratio\"],\n",
    "    random_state=RANDOM_SEED,\n",
    ")\n",
    "optimised_models.append(optimised_elkanoto)\n",
    "data = {\n",
    "    \"model\": \"Classic Elkanoto\",\n",
    "    \"params\": best_params,\n",
    "    \"ll_score\": study.best_value,\n",
    "}\n",
    "models_info.append(data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Elkanoto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "\n",
    "def weighted_elkanoto_objective(trial):\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    base_clf_option = trial.suggest_categorical(\"base_clf_option\", BASE_CLASSIFIERS)\n",
    "    best_classifier = best_base_classifier(base_clf_option)\n",
    "    if base_clf_option not in CALIBRATED_CLASSIFIERS:\n",
    "        best_classifier = CalibratedClassifierCV(\n",
    "            best_classifier, method=\"sigmoid\", cv=10\n",
    "        )\n",
    "    classifier_obj = WeightedElkanotoPuClassifier(\n",
    "        estimator=best_classifier,\n",
    "        hold_out_ratio=trial.suggest_float(\"hold_out_ratio\", 0.1, 0.8),\n",
    "        # Requires cardinality of labeled and unlabeled data\n",
    "        labeled=sum(y_train == 1),\n",
    "        unlabeled=sum(y_train == 0),\n",
    "    )\n",
    "    classifier_obj.fit(X_train_scaled, y_train)\n",
    "    y_pred = classifier_obj.predict(X_val_scaled)\n",
    "    return lee_liu_score(y_known=y_val, y_pred=y_pred)\n",
    "\n",
    "\n",
    "sampler = TPESampler(seed=RANDOM_SEED)\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "study.optimize(weighted_elkanoto_objective, n_trials=NUM_TRIALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_params\n",
    "best_clf_option = best_params[\"base_clf_option\"]\n",
    "best_classifier = best_base_classifier(best_clf_option)\n",
    "if best_clf_option not in CALIBRATED_CLASSIFIERS:\n",
    "    print(\"Calibating...\")\n",
    "    best_classifier = CalibratedClassifierCV(best_classifier, method=\"sigmoid\", cv=10)\n",
    "optimised_welkanoto = WeightedElkanotoPuClassifier(\n",
    "    estimator=best_classifier,\n",
    "    hold_out_ratio=best_params[\"hold_out_ratio\"],\n",
    "    labeled=sum(y == 1),\n",
    "    unlabeled=sum(y == 0),\n",
    "    random_state=RANDOM_SEED,\n",
    ")\n",
    "optimised_models.append(optimised_welkanoto)\n",
    "data = {\n",
    "    \"model\": \"Weighted Elkanoto\",\n",
    "    \"params\": best_params,\n",
    "    \"ll_score\": study.best_value,\n",
    "}\n",
    "models_info.append(data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "\n",
    "def bagging_objective(trial):\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    base_clf_option = trial.suggest_categorical(\"base_clf_option\", BASE_CLASSIFIERS)\n",
    "    best_classifier = best_base_classifier(base_clf_option)\n",
    "    classifier_obj = BaggingPuClassifier(\n",
    "        base_estimator=best_classifier,\n",
    "        n_estimators=trial.suggest_int(\"num_bagged\", 25, 200),\n",
    "        max_samples=trial.suggest_float(\"max_samples\", 0.1, 1.0),\n",
    "        max_features=trial.suggest_float(\"max_features\", 0.1, 1.0),\n",
    "    )\n",
    "    classifier_obj.fit(X_train_scaled, y_train)\n",
    "    y_pred = classifier_obj.predict(X_val_scaled)\n",
    "    return lee_liu_score(y_known=y_val, y_pred=y_pred)\n",
    "\n",
    "\n",
    "sampler = TPESampler(seed=RANDOM_SEED)\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "study.optimize(bagging_objective, n_trials=NUM_TRIALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_params\n",
    "print(f\"{best_params=}\")\n",
    "optimised_bagging = BaggingPuClassifier(\n",
    "    base_estimator=best_base_classifier(best_params[\"base_clf_option\"]),\n",
    "    n_estimators=best_params[\"num_bagged\"],\n",
    "    max_samples=best_params[\"max_samples\"],\n",
    "    max_features=best_params[\"max_features\"],\n",
    "    random_state=RANDOM_SEED,\n",
    ")\n",
    "optimised_models.append(optimised_bagging)\n",
    "data = {\n",
    "    \"model\": \"Bagging Classifier\",\n",
    "    \"params\": best_params,\n",
    "    \"ll_score\": study.best_value,\n",
    "}\n",
    "models_info.append(data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlr_objective(trial):\n",
    "    classifier_obj = ModifiedLogisticRegression(\n",
    "        epochs=250,\n",
    "        learning_rate=trial.suggest_float(\"learning_rate\", 1e-4, 1e-1),\n",
    "    )\n",
    "    classifier_obj.fit(X_train_scaled, y_train)\n",
    "    y_pred = classifier_obj.predict(X_val_scaled)\n",
    "    return lee_liu_score(y_known=y_val, y_pred=y_pred)\n",
    "\n",
    "\n",
    "sampler = TPESampler(seed=RANDOM_SEED)\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "study.optimize(mlr_objective, n_trials=NUM_TRIALS)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(f\"{best_params=}\")\n",
    "optimised_mlr = ModifiedLogisticRegression(epochs=250, **best_params)\n",
    "optimised_models.append(optimised_mlr)\n",
    "data = {\"model\": \"MLR\", \"params\": best_params, \"ll_score\": study.best_value}\n",
    "models_info.append(data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PUExtraTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import c_to_alpha\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "\n",
    "def puet_objective(trial):\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    classifier_obj = PUExtraTrees(\n",
    "        n_estimators=trial.suggest_int(\"n_estimators\", 25, 200),\n",
    "        risk_estimator=trial.suggest_categorical(\"risk_estimator\", [\"nnPU\", \"uPU\"]),\n",
    "        loss=trial.suggest_categorical(\"loss\", [\"quadratic\", \"logistic\"]),\n",
    "        min_samples_leaf=trial.suggest_int(\"min_samples_leaf\", 1, 10),\n",
    "        max_features=trial.suggest_categorical(\"max_features\", [\"sqrt\", \"all\"]),\n",
    "        max_candidates=trial.suggest_int(\"max_candidates\", 1, 10),\n",
    "    )\n",
    "\n",
    "    optimised_elkanoto.fit(X_train_scaled, y_train)\n",
    "    elkan_c = optimised_elkanoto.c\n",
    "    elkan_alpha = c_to_alpha(y, elkan_c)\n",
    "\n",
    "    classifier_obj.fit(X_train_scaled, y_train, alpha=elkan_alpha)\n",
    "    y_pred = classifier_obj.predict(X_val_scaled)\n",
    "    return lee_liu_score(y_known=y_val, y_pred=y_pred)\n",
    "\n",
    "\n",
    "sampler = TPESampler(seed=RANDOM_SEED)\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "study.optimize(puet_objective, n_trials=NUM_TRIALS)\n",
    "\n",
    "best_params = study.best_params\n",
    "optimised = PUExtraTrees(**best_params)\n",
    "optimised_models.append(optimised)\n",
    "data = {\"model\": \"PUExtraTrees\", \"params\": best_params, \"ll_score\": study.best_value}\n",
    "models_info.append(data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Look at the LL scores of the optimised models on this particular train-test split.\n",
    "# This is not likely to be a fair representation of the models\n",
    "models_df = pd.DataFrame(models_info)\n",
    "models_df.sort_values(\"ll_score\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeater Candidate Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(RANDOM_SEED)\n",
    "NUM_ITERATIONS = 1000\n",
    "MIN_VOTES_FOR_REPEATER_CANDIDATE = 3\n",
    "\n",
    "models_fit_predict_times = [[] for _ in range(len(optimised_models))]\n",
    "models_recalls = [[] for _ in range(len(optimised_models))]\n",
    "models_llscores = [[] for _ in range(len(optimised_models))]\n",
    "models_frac_pos = [[] for _ in range(len(optimised_models))]\n",
    "models_priors = [[] for _ in range(len(optimised_models))]\n",
    "\n",
    "candidates = {}\n",
    "\n",
    "# Save model and SHAP values from the last iteration\n",
    "explainers = []\n",
    "models_shap_values = []\n",
    "\n",
    "for j in range(NUM_ITERATIONS):\n",
    "    if j % (NUM_ITERATIONS * 0.05) == 0:\n",
    "        print(f\"Trial {j} / {NUM_ITERATIONS}\")\n",
    "\n",
    "    # Get training data for this iteration and rescale it\n",
    "    X_train, _, y_train, _ = get_subburst_preserved_train_test(\n",
    "        original_df=df, X=X, y=y, test_size=0.2\n",
    "    )\n",
    "    X_train = X_train.reset_index(drop=True)\n",
    "    y_train = y_train.reset_index(drop=True)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_scaled = scaler.transform(X)\n",
    "    X_scaled_df = pd.DataFrame(X_scaled, columns=FEATURES)\n",
    "\n",
    "    models_predictions = []\n",
    "    round_priors = []\n",
    "\n",
    "    for i, model in enumerate(optimised_models):\n",
    "        model_name = model.__class__.__name__\n",
    "\n",
    "        start = time.time()\n",
    "        if model_name == \"PUExtraTrees\":\n",
    "            puet_c_val = optimised_elkanoto.c\n",
    "            model.fit(X=X_train_scaled, y=y_train, alpha=c_to_alpha(y, puet_c_val))\n",
    "        else:\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "\n",
    "        # Save metrics and evaluation score\n",
    "        predictions = model.predict(X_scaled)\n",
    "        models_predictions.append(predictions)\n",
    "        models_fit_predict_times[i].append(time.time() - start)\n",
    "        models_recalls[i].append(recall_score(y, predictions))\n",
    "        models_llscores[i].append(lee_liu_score(y, predictions))\n",
    "        frac_pos = sum(predictions == 1) / len(predictions)\n",
    "        models_frac_pos[i].append(frac_pos)\n",
    "\n",
    "        # Save the prior for models which estiamte it\n",
    "        if model_name in [\n",
    "            \"ElkanotoPuClassifier\",\n",
    "            \"WeightedElkanotoPuClassifier\",\n",
    "            \"ModifiedLogisticRegression\",\n",
    "            \"PUExtraTrees\",\n",
    "        ]:\n",
    "            if (\n",
    "                model_name == \"ElkanotoPuClassifier\"\n",
    "                or model_name == \"WeightedElkanotoPuClassifier\"\n",
    "            ):\n",
    "                c = model.c\n",
    "                prior = c_to_alpha(y_train, c)\n",
    "            elif model_name == \"ModifiedLogisticRegression\":\n",
    "                c = model.c_hat[0]\n",
    "                prior = c_to_alpha(y_train, c)\n",
    "            elif model_name == \"PUExtraTrees\":\n",
    "                prior = c_to_alpha(y_train, puet_c_val)\n",
    "            models_priors[i].append(prior)\n",
    "\n",
    "        # Save SHAP values for each model in the last iteration\n",
    "        if j == NUM_ITERATIONS - 1:\n",
    "            explainer = shap.Explainer(model.predict, X_scaled)\n",
    "            explainer.feature_names = FEATURE_LABELS\n",
    "            explainers.append(explainer)\n",
    "            shap_values = explainer(X_scaled)\n",
    "            models_shap_values.append(shap_values)\n",
    "\n",
    "    # Get predictions on the full dataset\n",
    "    avg_preds = np.mean(models_predictions, axis=0)\n",
    "    flagged = np.where(\n",
    "        avg_preds > (MIN_VOTES_FOR_REPEATER_CANDIDATE / len(optimised_models))\n",
    "    )[0]\n",
    "\n",
    "    # Loop through the flagged indices and identify repeater candidates\n",
    "    for index in flagged:\n",
    "        if df.iloc[index][\"is_repeater\"] == 1:\n",
    "            continue\n",
    "        tns_name = df.iloc[index][\"tns_name\"]\n",
    "        sub_num = df.iloc[index][\"sub_num\"]\n",
    "        key = f\"{tns_name}_{sub_num}\"\n",
    "        if key not in candidates:\n",
    "            candidates[key] = 0\n",
    "        candidates[key] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_df[\"mean_fit_time\"] = [\n",
    "    np.mean(fittimes) for fittimes in models_fit_predict_times\n",
    "]\n",
    "models_df[\"std_fit_time\"] = [np.std(fittimes) for fittimes in models_fit_predict_times]\n",
    "models_df[\"mean_recall\"] = [np.mean(recalls) for recalls in models_recalls]\n",
    "models_df[\"std_recall\"] = [np.std(recalls) for recalls in models_recalls]\n",
    "models_df[\"mean_ll_score\"] = [np.mean(ll_scores) for ll_scores in models_llscores]\n",
    "models_df[\"std_ll_score\"] = [np.std(ll_scores) for ll_scores in models_llscores]\n",
    "models_df[\"frac_pos\"] = [np.mean(frac_pos) for frac_pos in models_frac_pos]\n",
    "models_df[\"std_frac_pos\"] = [np.std(frac_pos) for frac_pos in models_frac_pos]\n",
    "models_df[\"mean_prior\"] = [np.mean(priors) for priors in models_priors]\n",
    "models_df[\"std_prior\"] = [np.std(priors) for priors in models_priors]\n",
    "\n",
    "models_df.sort_values(by=\"mean_ll_score\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_df.sort_values(by=\"mean_ll_score\", ascending=False)[\n",
    "    [\"model\", \"mean_recall\", \"std_recall\", \"mean_ll_score\", \"std_ll_score\"]\n",
    "].to_latex(\"tables/puml_models_optimised.tex\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A sub-burst must be flagged in 10% of iterations to be considered a repeater candidate\n",
    "MIN_ITERS_REPEATER_CANDIDATE = 0.1 * NUM_ITERATIONS\n",
    "\n",
    "# Process the candidates into a dataframe\n",
    "candidates_df = pd.DataFrame.from_dict(candidates, columns=[\"count\"], orient=\"index\")\n",
    "candidates_df[\"tns_name\"] = candidates_df.index\n",
    "candidates_df = candidates_df[[\"tns_name\", \"count\"]]\n",
    "candidates_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "for index, row in candidates_df.iterrows():\n",
    "    # Convert the underscore format of the tns_name back to a split tns_name and sub_num\n",
    "    if row[\"tns_name\"][-2] == \"_\":\n",
    "        tns_name = row[\"tns_name\"][:-2]\n",
    "        sub_num = int(row[\"tns_name\"][-1:])\n",
    "        candidates_df.loc[index, \"tns_name\"] = tns_name\n",
    "        candidates_df.loc[index, \"sub_num\"] = int(sub_num)\n",
    "\n",
    "# Filter out candidates that were not flagged in the required fraction of predictions\n",
    "candidates_df.to_csv(\"./data/candidates/all_candidates.csv\", index=False)\n",
    "print(f\"Number of sub-bursts flagged at least once: {len(candidates_df)}\")\n",
    "candidates_df = candidates_df[candidates_df[\"count\"] >= (MIN_ITERS_REPEATER_CANDIDATE)]\n",
    "print(f\"Number of sub-burst candidates with >=10% confidence: {len(candidates_df)}\")\n",
    "candidates_df.sort_values(by=\"count\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total sources identified:\", candidates_df[\"tns_name\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set initial column values\n",
    "df[\"confidence\"] = 1\n",
    "df[\"is_candidate\"] = 0\n",
    "df[\"is_ref_candidate\"] = 0\n",
    "df[\"overlapping_papers\"] = \"\"\n",
    "df[\"num_overlapping\"] = 0\n",
    "\n",
    "# Loop through the candidates df and update the original dataframe with the right flags\n",
    "for index, row in candidates_df.iterrows():\n",
    "    tns_name = row[\"tns_name\"]\n",
    "    sub_num = row[\"sub_num\"]\n",
    "    original_row = df[(df[\"tns_name\"] == tns_name) & (df[\"sub_num\"] == sub_num)]\n",
    "\n",
    "    df.loc[original_row.index, \"is_candidate\"] = 1\n",
    "    df.loc[original_row.index, \"confidence\"] = row[\"count\"] / NUM_ITERATIONS\n",
    "\n",
    "# Note whether that sub-burst was flagged by previous ML papers as a repeater candidate\n",
    "for index, row in df.iterrows():\n",
    "    tns_name = row[\"tns_name\"]\n",
    "    if tns_name in REFERENCE_PAPER_REPEATERS:\n",
    "        df.loc[index, \"is_ref_candidate\"] = 1\n",
    "\n",
    "    # Now add a string indicating which papers it overlapped with\n",
    "    results = []\n",
    "    if tns_name in MORPHOLOGY_REPEATERS:  # Pleunis et al. 2021\n",
    "        results.append(1)\n",
    "    if tns_name in SUPERVISED_PAPER_REPEATERS:  # Luo et al. 2022\n",
    "        results.append(2)\n",
    "    if tns_name in UNSUPERVISED_PAPER_REPEATERS:  # Zhu-Ge et al. 20222\n",
    "        results.append(3)\n",
    "    result_str = \"\".join(\n",
    "        [\n",
    "            f\"{res}, \" if i < (len(results) - 1) else str(res)\n",
    "            for i, res in enumerate(results)\n",
    "        ]\n",
    "    )\n",
    "    df.loc[index, \"overlapping_papers\"] = result_str\n",
    "    df.loc[index, \"num_overlapping\"] = len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export particular columns to a latex table for appendix\n",
    "\n",
    "df[\"count\"] = df[\"confidence\"] * NUM_ITERATIONS\n",
    "export_df = df[df[\"is_candidate\"] == 1].sort_values(\n",
    "            [\"count\", \"num_overlapping\", \"tns_name\", \"sub_num\", \"is_pcc_candidate\"],\n",
    "            ascending=[False, False, False, False, True]\n",
    ")[[\"tns_name\", \"sub_num\"] + [\"count\", \"is_pcc_candidate\", \"overlapping_papers\"]]\n",
    "export_df[\"count\"] = export_df[\"count\"].astype(int)\n",
    "export_df[\"is_pcc_candidate\"] = export_df[\"is_pcc_candidate\"].apply(\n",
    "    lambda x: r\"\\checkmark\" if x == 1 else r\"\\xmark\"\n",
    ")\n",
    "export_df.columns = [\"TNS Name\", \"Sub-burst\"] + [\"Count\", \"From silver sample\", \"Overlapping with\"]\n",
    "export_df.to_latex(\"tables/candidates.tex\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"is_candidate\"] == 1][\"count\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the SHAP explainable AI library to evaluate feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_pred_function(row):\n",
    "    \"\"\"\n",
    "    Get predictions from the 1000th iteration of all the classifiers, and average them. Use this for SHAP analysis.\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    # These are the same as the model objects from the last round of training. Therefore, the predictions will be the same as in the last round\n",
    "    for model in optimised_models:\n",
    "        preds.append(model.predict(row))\n",
    "    mean_preds = np.mean(preds, axis=0)\n",
    "    return [round(x) for x in mean_preds]\n",
    "\n",
    "\n",
    "avg_explainer = shap.Explainer(avg_pred_function, X_scaled, random_state=RANDOM_SEED)\n",
    "avg_explainer.feature_names = FEATURE_LABELS\n",
    "avg_shap_values = avg_explainer(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "shap.plots.beeswarm(\n",
    "    shap_values=avg_shap_values,\n",
    "    max_display=len(FEATURES),\n",
    "    color=custom_cmap,\n",
    "    show=False,\n",
    ")\n",
    "plt.xlabel(\"SHAP value\")\n",
    "plt.ylabel(\"Feature\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./figures/puml/beeswarm_averaged.png\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FI for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mean SHAP importance values for each feature across all models\n",
    "exparr = np.array([abs(shap_values.values) for shap_values in models_shap_values])\n",
    "mean_importances = np.mean(np.mean(exparr, axis=1), axis=0)\n",
    "\n",
    "# Get the mean SHAP importance values for each feature for each model, to be plotted on a stacked bar chart\n",
    "importances_info = []\n",
    "for i, model_name in enumerate([\"CE\", \"WE\", \"BC\", \"MLR\", \"PUET\"]):\n",
    "    importances = np.mean(abs(models_shap_values[i].values), axis=0)\n",
    "    for i, (name, importance) in enumerate(zip(FEATURE_LABELS, importances)):\n",
    "        importances_info.append(\n",
    "            {\n",
    "                \"Classifier\": model_name,\n",
    "                \"Feature\": name,\n",
    "                \"Importance\": importance,\n",
    "                \"Mean importance\": mean_importances[i],\n",
    "            }\n",
    "        )\n",
    "# Sorting by mean importance means that the same features will be grouped together\n",
    "importances_info = pd.DataFrame(importances_info).sort_values(\n",
    "    \"Mean importance\", ascending=False\n",
    ")\n",
    "\n",
    "sns.set_palette(\"colorblind\")\n",
    "plt.figure(figsize=(3, 6))  # increase the height of the plot so that bars are thicker\n",
    "ax = sns.barplot(\n",
    "    data=importances_info, x=\"Importance\", y=\"Feature\", hue=\"Classifier\", linewidth=2.5\n",
    ")\n",
    "plt.xlabel(\"Importance (mean SHAP value)\", fontsize=12)\n",
    "plt.ylabel(\"Feature\", fontsize=12)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)\n",
    "\n",
    "# Add mean importance to next to each stacked bar plot\n",
    "ax.set_xlim(0, 0.24)\n",
    "for i, imp in enumerate(sorted(mean_importances, reverse=True)):\n",
    "    plt.text(0.21, i, round(imp, 3), ha=\"center\", va=\"center\")\n",
    "\n",
    "# Add light borders between each feature\n",
    "for i in range(len(FEATURES)):\n",
    "    plt.axhline(i + 0.5, color=\"lightgray\", linewidth=1)\n",
    "\n",
    "plt.savefig(\"./figures/puml/all_feature_importances.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Candidate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Resort so that the correct bursts are at the top in the plots\n",
    "df.sort_values(\n",
    "    by=[\"is_candidate\", \"is_ref_candidate\", \"is_pcc_candidate\", \"is_repeater\"],\n",
    "    inplace=True,\n",
    ")\n",
    "X = df[FEATURES]\n",
    "\n",
    "# Rescale and transform the entire dataset for visualisation in UMAP space\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "X_embedded = UMAP(random_state=RANDOM_SEED, n_neighbors=20).fit_transform(X_scaled)\n",
    "sns.set_palette(\"colorblind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df21 = pd.read_csv('./data/raw_data/chime_frb_catalog_2021.csv')\n",
    "tns_21 = list(df21['tns_name'])\n",
    "df['is_2021'] = df.apply(lambda x: True if x['tns_name'] in tns_21 else False, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LINE_WIDTH = 0.375"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot candidates on scatterplot\n",
    "plt.clf()\n",
    "plot = sns.scatterplot(\n",
    "    x=X_embedded[:, 0],\n",
    "    y=X_embedded[:, 1],\n",
    "    hue=df.apply(\n",
    "        lambda row: 0\n",
    "        if row[\"is_candidate\"] == 1\n",
    "        else 1\n",
    "        if row[\"is_repeater\"] == 1\n",
    "        else 2,\n",
    "        axis=1,\n",
    "    ),\n",
    "    style=df.apply(\n",
    "        lambda row: 0\n",
    "        if row[\"is_candidate\"] == 1\n",
    "        else 1\n",
    "        if row[\"is_repeater\"] == 1\n",
    "        else 2,\n",
    "        axis=1,\n",
    "    ),\n",
    "    markers={0: \"X\", 1: \"o\", 2: \"o\"},\n",
    "    palette={0: \"#0173b2\", 1: \"#cc78bc\", 2: \"#029e73\"},\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=LINE_WIDTH,\n",
    ")\n",
    "\n",
    "handles, _ = plot.get_legend_handles_labels()\n",
    "plot.legend(\n",
    "    handles=handles,\n",
    "    labels=[\"Candidate\", \"Known repeater\", \"Non-repeater\"],\n",
    "    loc=(0, -0.2),\n",
    ")\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\n",
    "    f\"./figures/puml/visualisations/simple-UMAP-candidates.png\",\n",
    "    dpi=500,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with previous work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we compare the candidates we flagged with the results of [Luo et al (2022)](https://arxiv.org/abs/2210.02463), [Zhu-Ge et al. (2023)](https://arxiv.org/abs/2210.02471), and [Pleunis et al (2021)](https://iopscience.iop.org/article/10.3847/1538-4357/ac33ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These list the number of sub-bursts, i.e, the points on the plot\n",
    "df.apply(\n",
    "    lambda row: 0\n",
    "    if row[\"is_candidate\"] == 1 and row[\"is_ref_candidate\"] == 1\n",
    "    else 1\n",
    "    if row[\"is_candidate\"] == 1\n",
    "    else 2\n",
    "    if row[\"is_ref_candidate\"] == 1\n",
    "    else 3\n",
    "    if row[\"is_repeater\"] == 1\n",
    "    else 4,\n",
    "    axis=1,\n",
    ").value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by sub-bursts which the previous papers had available to them\n",
    "df[df.apply(lambda row: '202' not in row[\"tns_name\"], axis=1)].apply(\n",
    "    lambda row: 0 if row[\"is_candidate\"] == 1 and row[\"is_ref_candidate\"] == 1\n",
    "    else 1 if row[\"is_candidate\"] == 1\n",
    "    else 2 if row[\"is_ref_candidate\"] == 1\n",
    "    else 3,\n",
    "    axis=1,\n",
    ").value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the bursts which we identify to be new repeating sources that the previous papers did not\n",
    "new_sources = df[\n",
    "    (df[\"is_ref_candidate\"] == 0)\n",
    "    & (df[\"is_candidate\"] == 1)\n",
    "    & (df.apply(lambda row: \"202\" not in row[\"tns_name\"], axis=1))\n",
    "][\"tns_name\"].unique()\n",
    "print(\n",
    "    f\"{len(new_sources)} new sources not identified by reference papers:\", new_sources\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the bursts which they identify to be repeating sources that we do not \n",
    "missed_sources = df[\n",
    "    (df[\"is_ref_candidate\"] == 1)\n",
    "    & (df[\"is_candidate\"] == 0)\n",
    "][\"tns_name\"].unique()\n",
    "print(\n",
    "    f\"{len(missed_sources)} sources identified by reference papers but not by us:\", missed_sources\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for palette\n",
    "PALETTE_HEX = {0: \"#0173b2\", 1: \"#029e73\", 2: \"#fbafe4\", 3: \"#de8f05\", 4: \"#949494\"}\n",
    "EXPANDED_PALETTE_RGBA = {0: (1/255, 115/255, 178/255, 1.0),\n",
    "        1: (2/255, 158/255, 115/255, 1.0),\n",
    "        2: (251/255, 175/255, 228/255, 1.0),\n",
    "        3: (222/255, 143/255, 5/255, 1.0),\n",
    "        4: (148/255, 148/255, 148/255, 1.0),\n",
    "        5: (1/255, 115/255, 178/255, 0.4),\n",
    "        6: (2/255, 158/255, 115/255, 0.4),\n",
    "        7: (251/255, 175/255, 228/255, 0.4),\n",
    "        8: (222/255, 143/255, 5/255, 0.4),\n",
    "        9: (148/255, 148/255, 148/255, 0.4),\n",
    "}\n",
    "EXPANDED_EDGE_COLORS = {0: \"black\", 1: \"black\", 2: \"black\", 3: \"black\", 4: \"black\", 5: \"none\", 6: \"none\", 7: \"none\", 8: \"none\", 9: \"none\"}\n",
    "MARKERS = {0: \"X\", 1: \"D\", 2: \"v\", 3: \"o\", 4: \"*\"}\n",
    "ICON_SIZE_STD = 50\n",
    "LEGEND_LOC = (0, -0.3)\n",
    "\n",
    "def gen_legend_handles(labels: list[str], markers=MARKERS, palette:dict=PALETTE_HEX, markersize=ICON_SIZE_STD * 0.15):\n",
    "        \"\"\"\n",
    "        Generate legend handles for a scatterplot\n",
    "        \"\"\"\n",
    "        return [\n",
    "                Line2D(\n",
    "                [0],\n",
    "                [0],\n",
    "                marker=markers[i],\n",
    "                color=PALETTE_HEX[i],\n",
    "                markerfacecolor=palette[i],\n",
    "                markersize=markersize,\n",
    "                linestyle=\"None\",\n",
    "                label=label,\n",
    "                )\n",
    "                for i, label in enumerate(labels)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "\n",
    "hue_style_values = df.apply(\n",
    "        lambda row: 0 if row[\"is_candidate\"] == 1 and row[\"is_ref_candidate\"] == 1 and row[\"is_2021\"] == 1 \n",
    "        else 1 if row[\"is_candidate\"] == 1 and row[\"is_2021\"] == 1  \n",
    "        else 2 if row[\"is_ref_candidate\"] == 1 and row[\"is_2021\"] == 1 \n",
    "        else 3 if row[\"is_repeater\"] == 1 and row[\"is_2021\"] == 1 \n",
    "        else 4 if row[\"is_2021\"] == 1\n",
    "\n",
    "        else 5 if row[\"is_candidate\"] == 1 and row[\"is_ref_candidate\"] == 1 and row[\"is_2021\"] == 0\n",
    "        else 6 if row[\"is_candidate\"] == 1 and row[\"is_2021\"] == 0\n",
    "        else 7 if row[\"is_ref_candidate\"] == 1 and row[\"is_2021\"] == 0\n",
    "        else 8 if row[\"is_repeater\"] == 1 and row[\"is_2021\"] == 0\n",
    "        else 9,\n",
    "        axis=1,\n",
    ")\n",
    "style_values = df.apply(\n",
    "        lambda row: 0 if row[\"is_candidate\"] == 1 and row[\"is_ref_candidate\"] == 1\n",
    "        else 1 if row[\"is_candidate\"] == 1\n",
    "        else 2 if row[\"is_ref_candidate\"] == 1\n",
    "        else 3 if row[\"is_repeater\"] == 1\n",
    "        else 4,\n",
    "        axis=1,\n",
    ")\n",
    "plot = sns.scatterplot(\n",
    "    x=X_embedded[:, 0],\n",
    "    y=X_embedded[:, 1],\n",
    "\n",
    "    style=style_values,\n",
    "    markers=MARKERS,\n",
    "    linewidth=LINE_WIDTH,\n",
    "\n",
    "    hue=hue_style_values,\n",
    "    palette=EXPANDED_PALETTE_RGBA,\n",
    "    edgecolor=hue_style_values.map(EXPANDED_EDGE_COLORS),\n",
    "\n",
    "    # Adjust for a visual oddity that results in the stars without borders appearing too small\n",
    "    size=(df.apply(lambda row: ICON_SIZE_STD*1.6 if (row[\"is_repeater\"] == False and row[\"is_2021\"] == False and row[\"is_candidate\"] == False) else ICON_SIZE_STD, axis=1)),\n",
    "    sizes=(ICON_SIZE_STD, ICON_SIZE_STD*1.6),\n",
    "\n",
    ")\n",
    "handles, _  = plot.get_legend_handles_labels()\n",
    "plot.legend(\n",
    "    handles=gen_legend_handles(labels=[\n",
    "        \"Overlapping candidates\",\n",
    "        \"New candidates with PU learning\",\n",
    "        \"Candidates from previous work\",\n",
    "        \"Known repeaters\",\n",
    "        \"Non-repeaters\",\n",
    "    ]),\n",
    "    loc=LEGEND_LOC\n",
    ")\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\n",
    "    f\"./figures/puml/visualisations/UMAP-candidate_compare.png\",\n",
    "    dpi=500,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with silver sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can analyse the model's predictions on the 14 'silver sample' candidates identified by [CHIME/FRB 2023](https://iopscience.iop.org/article/10.3847/1538-4357/acc6c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "\n",
    "hue_style_values = df.apply(\n",
    "        lambda row: 0 if row[\"is_candidate\"] == 1 and row[\"is_pcc_candidate\"] == 1 and row[\"is_2021\"] == 0 \n",
    "        else 1 if row[\"is_candidate\"] == 1 and row[\"is_2021\"] == 0  \n",
    "        else 2 if row[\"is_pcc_candidate\"] == 1 and row[\"is_2021\"] == 0 \n",
    "        else 3 if row[\"is_repeater\"] == 1 and row[\"is_2021\"] == 0 \n",
    "        else 4 if row[\"is_2021\"] == 0\n",
    "\n",
    "        else 5 if row[\"is_candidate\"] == 1 and row[\"is_pcc_candidate\"] == 1 and row[\"is_2021\"] == 1\n",
    "        else 6 if row[\"is_candidate\"] == 1 and row[\"is_2021\"] == 1\n",
    "        else 7 if row[\"is_pcc_candidate\"] == 1 and row[\"is_2021\"] == 1\n",
    "        else 8 if row[\"is_repeater\"] == 1 and row[\"is_2021\"] == 1\n",
    "        else 9,\n",
    "        axis=1,\n",
    ")\n",
    "style_values = df.apply(\n",
    "        lambda row: 0 if row[\"is_candidate\"] == 1 and row[\"is_pcc_candidate\"] == 1\n",
    "        else 1 if row[\"is_candidate\"] == 1\n",
    "        else 2 if row[\"is_pcc_candidate\"] == 1\n",
    "        else 3 if row[\"is_repeater\"] == 1\n",
    "        else 4,\n",
    "        axis=1,\n",
    "    )\n",
    "plot = sns.scatterplot(\n",
    "    x=X_embedded[:, 0],\n",
    "    y=X_embedded[:, 1],\n",
    "\n",
    "    style=style_values,\n",
    "    markers=MARKERS,\n",
    "    linewidth=LINE_WIDTH,\n",
    "\n",
    "    hue=hue_style_values,\n",
    "    palette=EXPANDED_PALETTE_RGBA,\n",
    "    edgecolor=hue_style_values.map(EXPANDED_EDGE_COLORS),\n",
    ")\n",
    "plt.legend(handles=gen_legend_handles(labels=[\n",
    "                \"Overlapping candidates\",\n",
    "                \"New candidates with PU learning\",\n",
    "                \"Silver sample candidate\",\n",
    "                \"Known repeaters\",\n",
    "                \"Non-repeaters\"\n",
    "           ]),\n",
    "           loc=LEGEND_LOC,\n",
    ")\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(    \n",
    "    f\"./figures/puml/visualisations/UMAP-2023_chime_candidates.png\",\n",
    "    dpi=500,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we compare our confidence to the $R_{CC}$ value. We should ideally see an inverse correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chime_frb_2023_silver = pd.read_csv(\"./data/candidates/2023_chimefrb_silver.csv\")\n",
    "all_candidates_df = pd.read_csv(\"./data/candidates/all_candidates.csv\")\n",
    "\n",
    "for index, row in chime_frb_2023_silver.iterrows():\n",
    "    # Check if we identified the exact same sub-burst\n",
    "    tns_name = row[\"tns_name\"]\n",
    "    sub_num = row[\"sub_num\"]\n",
    "    try:\n",
    "        df_row = all_candidates_df[\n",
    "            (all_candidates_df[\"tns_name\"] == tns_name)\n",
    "            & (all_candidates_df[\"sub_num\"] == sub_num)\n",
    "        ]\n",
    "        confidence = df_row[\"count\"].values[0] / NUM_ITERATIONS\n",
    "    except Exception as e:\n",
    "        # Not identified as candidate\n",
    "        confidence = 0\n",
    "\n",
    "    chime_frb_2023_silver.loc[index, \"confidence\"] = confidence\n",
    "chime_frb_2023_silver[\"R_cc_recip\"] = 1 / chime_frb_2023_silver[\"R_cc\"]\n",
    "chime_frb_2023_silver[\n",
    "    [\"tns_name\", \"repeater_name\", \"confidence\", \"R_cc\", \"R_cc_recip\"]\n",
    "].sort_values([\"confidence\", \"R_cc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chime_frb_2023_silver[chime_frb_2023_silver[\"confidence\"] > 0][\n",
    "    \"repeater_name\"\n",
    "].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more generous approach would be to assume that, if we identify _any_ sub-burst in a cluster from the silver sample, we have identified the entire silver sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in chime_frb_2023_silver.iterrows():\n",
    "    repeater_name = row[\"repeater_name\"]\n",
    "\n",
    "    # Check if the first burst in the cluster is identified as a candidate\n",
    "    if repeater_name in list(all_candidates_df[\"tns_name\"]):\n",
    "        chime_frb_2023_silver.loc[index, \"confidence\"] = (\n",
    "            all_candidates_df[all_candidates_df[\"tns_name\"] == repeater_name][\n",
    "                \"count\"\n",
    "            ].values[0]\n",
    "            / NUM_ITERATIONS\n",
    "        )\n",
    "\n",
    "    # Find all the other rows with the same repeater name in the silver sample\n",
    "    other_rows = chime_frb_2023_silver[\n",
    "        chime_frb_2023_silver[\"repeater_name\"] == repeater_name\n",
    "    ]\n",
    "    for other_idx, other_row in other_rows.iterrows():\n",
    "        tns_name = other_row[\"tns_name\"]\n",
    "        sub_num = other_row[\"sub_num\"]\n",
    "        if tns_name in list(all_candidates_df[\"tns_name\"]):\n",
    "            chime_frb_2023_silver.loc[index, \"confidence\"] = (\n",
    "                all_candidates_df[all_candidates_df[\"tns_name\"] == tns_name][\n",
    "                    \"count\"\n",
    "                ].values[0]\n",
    "                / NUM_ITERATIONS\n",
    "            )\n",
    "\n",
    "chime_frb_2023_silver[\n",
    "    [\"tns_name\", \"repeater_name\", \"confidence\", \"R_cc\", \"R_cc_recip\"]\n",
    "].sort_values(\n",
    "    [\n",
    "        \"confidence\",\n",
    "        \"repeater_name\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silver_overlapping_candidates = chime_frb_2023_silver[\n",
    "    chime_frb_2023_silver[\"confidence\"] > 0\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now update the original df again\n",
    "copied_df = df.copy()\n",
    "for index, row in copied_df.iterrows():\n",
    "    if row[\"tns_name\"] in silver_overlapping_candidates[\"tns_name\"].values:\n",
    "        if row[\"is_candidate\"] != 1:\n",
    "            copied_df.loc[index, \"is_candidate\"] = 1\n",
    "print(\"\\nOld:\")\n",
    "print(\n",
    "    df.apply(\n",
    "        lambda row: 0\n",
    "        if row[\"is_candidate\"] == 1 and row[\"is_pcc_candidate\"] == 1\n",
    "        else 1\n",
    "        if row[\"is_candidate\"] == 1\n",
    "        else 2\n",
    "        if row[\"is_pcc_candidate\"] == 1\n",
    "        else 3\n",
    "        if row[\"is_repeater\"] == 1\n",
    "        else 4,\n",
    "        axis=1,\n",
    "    ).value_counts()\n",
    ")\n",
    "print(\"\\nNew:\")\n",
    "print(\n",
    "    copied_df.apply(\n",
    "        lambda row: 0\n",
    "        if row[\"is_candidate\"] == 1 and row[\"is_pcc_candidate\"] == 1\n",
    "        else 1\n",
    "        if row[\"is_candidate\"] == 1\n",
    "        else 2\n",
    "        if row[\"is_pcc_candidate\"] == 1\n",
    "        else 3\n",
    "        if row[\"is_repeater\"] == 1\n",
    "        else 4,\n",
    "        axis=1,\n",
    "    ).value_counts()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this perspective, we've identified 18, not 10, overlapping sub-bursts. Below, we confirm below that the repeater candidates identified are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silver_overlapping_candidates[\"repeater_name\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "\n",
    "hue_style_values = copied_df.apply(\n",
    "        lambda row: 0 if row[\"is_candidate\"] == 1 and row[\"is_pcc_candidate\"] == 1 and row[\"is_2021\"] == 0 \n",
    "        else 1 if row[\"is_candidate\"] == 1 and row[\"is_2021\"] == 0  \n",
    "        else 2 if row[\"is_pcc_candidate\"] == 1 and row[\"is_2021\"] == 0 \n",
    "        else 3 if row[\"is_repeater\"] == 1 and row[\"is_2021\"] == 0 \n",
    "        else 4 if row[\"is_2021\"] == 0\n",
    "\n",
    "        else 5 if row[\"is_candidate\"] == 1 and row[\"is_pcc_candidate\"] == 1 and row[\"is_2021\"] == 1\n",
    "        else 6 if row[\"is_candidate\"] == 1 and row[\"is_2021\"] == 1\n",
    "        else 7 if row[\"is_pcc_candidate\"] == 1 and row[\"is_2021\"] == 1\n",
    "        else 8 if row[\"is_repeater\"] == 1 and row[\"is_2021\"] == 1\n",
    "        else 9,\n",
    "        axis=1,\n",
    ")\n",
    "style_values = copied_df.apply(\n",
    "        lambda row: 0 if row[\"is_candidate\"] == 1 and row[\"is_pcc_candidate\"] == 1\n",
    "        else 1 if row[\"is_candidate\"] == 1\n",
    "        else 2 if row[\"is_pcc_candidate\"] == 1\n",
    "        else 3 if row[\"is_repeater\"] == 1\n",
    "        else 4,\n",
    "        axis=1,\n",
    "    )\n",
    "plot = sns.scatterplot(\n",
    "    x=X_embedded[:, 0],\n",
    "    y=X_embedded[:, 1],\n",
    "\n",
    "    style=style_values,\n",
    "    markers=MARKERS,\n",
    "    linewidth=LINE_WIDTH,\n",
    "\n",
    "    hue=hue_style_values,\n",
    "    palette=EXPANDED_PALETTE_RGBA,\n",
    "    edgecolor=hue_style_values.map(EXPANDED_EDGE_COLORS),\n",
    ")\n",
    "plt.legend(handles=gen_legend_handles(labels=[\n",
    "                \"Overlapping candidates\",\n",
    "                \"New candidates with PU learning\",\n",
    "                \"Silver sample candidate\",\n",
    "                \"Known repeaters\",\n",
    "                \"Non-repeaters\"\n",
    "           ]),\n",
    "           loc=LEGEND_LOC,\n",
    ")\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\n",
    "    f\"./figures/puml/visualisations/UMAP-2023_chime_candidates-all.png\",\n",
    "    dpi=500,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try to better understand why the results are so different, by comparing the candidate repeaters and the silver sample repeaters using the k-sample Anderson-Darling test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import anderson_ksamp\n",
    "\n",
    "pvals = []\n",
    "\n",
    "for i, feature in enumerate(FEATURES):\n",
    "    res = anderson_ksamp(\n",
    "        samples=[\n",
    "            df[df[\"is_repeater\"] == 1][feature],\n",
    "            df[df[\"is_pcc_candidate\"] == 1][feature],\n",
    "        ]\n",
    "    )\n",
    "    rep_pcc_stat = res.statistic\n",
    "    rep_pcc_p_value = res.significance_level\n",
    "\n",
    "    res = anderson_ksamp(\n",
    "        [df[df[\"is_repeater\"] == 1][feature], df[df[\"is_candidate\"] == 1][feature]]\n",
    "    )\n",
    "    rep_cand_stat = res.statistic\n",
    "    rep_cand_p_value = res.significance_level\n",
    "\n",
    "    res = anderson_ksamp(\n",
    "        [\n",
    "            df[df[\"is_repeater\"] == 1][feature],\n",
    "            df[(df[\"is_pcc_candidate\"] == 1) & (df[\"is_candidate\"] == 0)][feature],\n",
    "        ]\n",
    "    )\n",
    "    rep_pcc_not_cand_stat = res.statistic\n",
    "    rep_pcc_not_cand_p_value = res.significance_level\n",
    "\n",
    "    pvals.append(\n",
    "        {\n",
    "            \"feature\": FEATURE_LABELS[i],\n",
    "            \"Candidates\": rep_cand_p_value,\n",
    "            \"RCC\": rep_pcc_p_value,\n",
    "            \"RCC Non-candidates\": rep_pcc_not_cand_p_value,\n",
    "        }\n",
    "    )\n",
    "pd.options.display.float_format = '{:.10f}'.format\n",
    "pd.DataFrame(pvals).to_latex(\"tables/ad_pvals.tex\", index=False)\n",
    "pd.DataFrame(pvals).head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
